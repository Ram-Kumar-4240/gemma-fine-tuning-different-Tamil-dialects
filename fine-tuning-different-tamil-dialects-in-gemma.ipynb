{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":85416,"databundleVersionId":9690815,"sourceType":"competition"},{"sourceId":11358,"sourceType":"modelInstanceVersion","modelInstanceId":5383,"modelId":3301}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install trl peft datasets huggingface_hub bitsandbytes accelerate wandb","metadata":{"execution":{"iopub.status.busy":"2024-10-15T11:17:17.125582Z","iopub.execute_input":"2024-10-15T11:17:17.126459Z","iopub.status.idle":"2024-10-15T11:17:36.173286Z","shell.execute_reply.started":"2024-10-15T11:17:17.126399Z","shell.execute_reply":"2024-10-15T11:17:36.172193Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Login in to huggingface","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"HUGGINGFACE_API_KEY\")\nfrom huggingface_hub import login\ntoken = secret_value_0\nlogin(token=token)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T11:17:36.175469Z","iopub.execute_input":"2024-10-15T11:17:36.175804Z","iopub.status.idle":"2024-10-15T11:17:36.929244Z","shell.execute_reply.started":"2024-10-15T11:17:36.175768Z","shell.execute_reply":"2024-10-15T11:17:36.928345Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Importing libraries","metadata":{}},{"cell_type":"code","source":"from transformers import (\n    AutoTokenizer, AutoModelForCausalLM, TrainingArguments,\n    DataCollatorForLanguageModeling\n)\nfrom peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-10-15T11:17:36.930491Z","iopub.execute_input":"2024-10-15T11:17:36.930881Z","iopub.status.idle":"2024-10-15T11:17:55.998231Z","shell.execute_reply.started":"2024-10-15T11:17:36.930836Z","shell.execute_reply":"2024-10-15T11:17:55.997427Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Define dataset name ","metadata":{}},{"cell_type":"code","source":"dataset_name = \"oscar\"  # OSCAR dataset with multilingual support\nlanguage = \"unshuffled_deduplicated_ta\"  # Language code for Tamil","metadata":{"execution":{"iopub.status.busy":"2024-10-15T11:17:55.999288Z","iopub.execute_input":"2024-10-15T11:17:55.999852Z","iopub.status.idle":"2024-10-15T11:17:56.004201Z","shell.execute_reply.started":"2024-10-15T11:17:55.999818Z","shell.execute_reply":"2024-10-15T11:17:56.00317Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"loading dataset","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(dataset_name, f\"{language}\", split=\"train[:1000]\",trust_remote_code=True)  # Subset of the Tamil dataset","metadata":{"execution":{"iopub.status.busy":"2024-10-15T11:17:56.007581Z","iopub.execute_input":"2024-10-15T11:17:56.007981Z","iopub.status.idle":"2024-10-15T11:20:23.679311Z","shell.execute_reply.started":"2024-10-15T11:17:56.007921Z","shell.execute_reply":"2024-10-15T11:20:23.678484Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"print the dataset samples","metadata":{}},{"cell_type":"code","source":"#  Dataset Length\ndataset_length = len(dataset)\nprint(f\"Length of the dataset: {dataset_length}\")\n#  Printing a few samples\nfor i in range(2):\n    print(f\"Sample {i + 1}: {dataset[i]['text']}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-15T11:20:23.680594Z","iopub.execute_input":"2024-10-15T11:20:23.680911Z","iopub.status.idle":"2024-10-15T11:20:23.689434Z","shell.execute_reply.started":"2024-10-15T11:20:23.680876Z","shell.execute_reply":"2024-10-15T11:20:23.688471Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This config enables memory-efficient 4-bit quantization for faster training.","metadata":{}},{"cell_type":"code","source":"from transformers import BitsAndBytesConfig\n\n# Set the BitsAndBytesConfig for 4-bit quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,  # Use 4-bit quantization\n    bnb_4bit_compute_dtype=torch.float16,  # Use mixed precision (FP16) during training\n    bnb_4bit_use_double_quant=True,  # Enable double quantization for memory efficiency\n    bnb_4bit_quant_type=\"nf4\"  # NF4 quantization type\n)\n\nbase_model = \"google/gemma-2-2b\"  \nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    device_map=\"auto\",\n    quantization_config=bnb_config \n)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T11:20:23.690549Z","iopub.execute_input":"2024-10-15T11:20:23.690849Z","iopub.status.idle":"2024-10-15T11:21:17.179314Z","shell.execute_reply.started":"2024-10-15T11:20:23.690815Z","shell.execute_reply":"2024-10-15T11:21:17.178299Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Disables caching and enables gradient checkpointing to save memory.","metadata":{}},{"cell_type":"code","source":"model.config.use_cache = False\nmodel.gradient_checkpointing_enable()","metadata":{"execution":{"iopub.status.busy":"2024-10-15T11:21:17.182847Z","iopub.execute_input":"2024-10-15T11:21:17.183488Z","iopub.status.idle":"2024-10-15T11:21:17.190504Z","shell.execute_reply.started":"2024-10-15T11:21:17.183449Z","shell.execute_reply":"2024-10-15T11:21:17.189683Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Initializes the tokenizer, sets padding side, and calculates token lengths.","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(base_model)\ntokenizer.padding_side = 'right'\ntokenizer.pad_token = tokenizer.eos_token\ntoken_lengths = [len(tokenizer(text['text'], truncation=True)['input_ids']) for text in dataset]","metadata":{"execution":{"iopub.status.busy":"2024-10-15T11:21:17.191564Z","iopub.execute_input":"2024-10-15T11:21:17.19251Z","iopub.status.idle":"2024-10-15T11:21:22.816777Z","shell.execute_reply.started":"2024-10-15T11:21:17.192453Z","shell.execute_reply":"2024-10-15T11:21:22.815855Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Plotting","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.hist(token_lengths, bins=30, color='blue')\nplt.title('Token Length Distribution')\nplt.xlabel('Token Length')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-15T11:21:22.81809Z","iopub.execute_input":"2024-10-15T11:21:22.818481Z","iopub.status.idle":"2024-10-15T11:21:23.174194Z","shell.execute_reply.started":"2024-10-15T11:21:22.818435Z","shell.execute_reply":"2024-10-15T11:21:23.173277Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The specified training parameters configure the model's training process effectively. **`per_device_train_batch_size=16`** sets the batch size for each GPU, while **`gradient_accumulation_steps=16`** allows for accumulating gradients over 16 batches to simulate a larger batch size without exceeding memory limits. The **`optim=\"adamw_torch\"`** specifies the AdamW optimizer, which improves convergence with weight decay to prevent overfitting (**`weight_decay=0.01`**). Checkpoints are saved every 100 steps (**`save_steps=100`**) with a limit of 2 saved checkpoints (**`save_total_limit=2`**). Logging is performed every 10 steps (**`logging_steps=10`**), and evaluation occurs at the end of each epoch (**`eval_strategy=\"epoch\"`**). A cosine learning rate schedule (**`lr_scheduler_type=\"cosine\"`**) and 500 warmup steps (**`warmup_steps=500`**) stabilize training, while **`fp16=False`** and **`bf16=True`** optimize performance with bfloat16 precision. **`gradient_checkpointing=True`** saves memory by reducing the storage of intermediate activations. Overall, these settings aim to balance efficiency, model performance, and resource management during training.","metadata":{}},{"cell_type":"code","source":"import os\n\n# Disable wandb logging\nos.environ[\"WANDB_DISABLED\"] = \"true\"\ntraining_args = TrainingArguments(\n    output_dir=\"./gemma-2b-oscar-tamil-finetuned\",\n    logging_dir=\"./logs\",\n    num_train_epochs=50,  \n    per_device_train_batch_size=16,\n    gradient_accumulation_steps=16,  \n    optim=\"adamw_torch\",\n    save_steps=100,  # Save after each epoch\n    save_total_limit=2,  # Keep only 2 checkpoints\n    logging_steps=10,\n    eval_strategy=\"epoch\",  # Evaluate after each epoch\n    lr_scheduler_type=\"cosine\",  # Cosine schedule for smooth learning rate decay\n    warmup_steps=500,\n    weight_decay=0.01,\n    fp16=False, # For better accuracy \n    bf16=True,\n    group_by_length=True,\n    report_to=None, \n    gradient_checkpointing=True,\n    learning_rate=2e-5,\n    warmup_ratio=0.1,\n    max_grad_norm=0.3,\n    push_to_hub=False  \n)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T11:30:03.672301Z","iopub.execute_input":"2024-10-15T11:30:03.672701Z","iopub.status.idle":"2024-10-15T11:30:03.705342Z","shell.execute_reply.started":"2024-10-15T11:30:03.672661Z","shell.execute_reply":"2024-10-15T11:30:03.704435Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The tokenize_function takes a batch of text examples and uses the tokenizer to convert them into token IDs, applying truncation to ensure that texts longer than 64 tokens are cut off, while shorter texts are padded to this maximum length. This standardization facilitates efficient training by ensuring consistent input sizes. The resulting tokenized dataset is created by mapping this function across the entire dataset. Additionally, the DataCollatorForLanguageModeling is instantiated to group the tokenized sequences into batches for training, with mlm=False indicating that a causal language modeling approach is used instead of masked language modeling. This setup prepares the data effectively for training a language model.","metadata":{}},{"cell_type":"code","source":"def tokenize_function(examples):\n    return tokenizer(\n        examples['text'], truncation=True, padding=\"max_length\", max_length=64  \n    )\n\ntokenized_dataset = dataset.map(tokenize_function, batched=True)\n\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=False  # We are doing causal LM, not masked LM\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T11:30:04.08344Z","iopub.execute_input":"2024-10-15T11:30:04.083773Z","iopub.status.idle":"2024-10-15T11:30:05.66264Z","shell.execute_reply.started":"2024-10-15T11:30:04.083738Z","shell.execute_reply":"2024-10-15T11:30:05.661553Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"LoRA (Low-Rank Adaptation) is used in models to reduce the number of trainable parameters by introducing low-rank matrices into specific layers (e.g., projection layers in transformers). This approach makes fine-tuning large models more memory-efficient and faster, especially for tasks like language modeling, without retraining the entire model. PEFT (Parameter-Efficient Fine-Tuning) refers to methods like LoRA that enable tuning only a small subset of parameters, improving efficiency. It's used to minimize resource consumption while still achieving performance close to full fine-tuning, making it ideal for adapting large-scale models to new tasks with limited computational power.","metadata":{}},{"cell_type":"code","source":"model = prepare_model_for_kbit_training(model)\n# LoRA configuration: \nlora_config = LoraConfig(\n    r=64,  # Rank of the low-rank matrix\n    lora_alpha=32,  # Scaling factor for LoRA typically start with alpha=r and go upto alpha=2r\n    lora_dropout=0.1,  # Dropout to avoid overfitting \n    target_modules=['o_proj', 'q_proj', 'up_proj', 'v_proj', 'k_proj', 'down_proj', 'gate_proj'],  # Apply LoRA to  projection layers in the transformer\n    task_type=\"CAUSAL_LM\"  #For casual language modelling\n)\n\n# Apply LoRA to the model using PEFT(parameter Efficient Fine Tuning)\nmodel = get_peft_model(model, lora_config)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T11:30:05.664749Z","iopub.execute_input":"2024-10-15T11:30:05.665264Z","iopub.status.idle":"2024-10-15T11:30:06.736185Z","shell.execute_reply.started":"2024-10-15T11:30:05.665215Z","shell.execute_reply":"2024-10-15T11:30:06.735168Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Trainer to train the gemma","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    train_dataset=tokenized_dataset,\n    eval_dataset=tokenized_dataset, \n    tokenizer=tokenizer,\n    args=training_args,\n    data_collator=data_collator,\n)\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-10-15T11:30:06.737685Z","iopub.execute_input":"2024-10-15T11:30:06.738148Z","iopub.status.idle":"2024-10-15T15:11:39.808418Z","shell.execute_reply.started":"2024-10-15T11:30:06.738103Z","shell.execute_reply":"2024-10-15T15:11:39.807619Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Saving the trained model","metadata":{}},{"cell_type":"code","source":"trainer.save_model(\"./gemma-2b-oscar-tamil-finetuned\")","metadata":{"execution":{"iopub.status.busy":"2024-10-15T15:11:39.810357Z","iopub.execute_input":"2024-10-15T15:11:39.810668Z","iopub.status.idle":"2024-10-15T15:11:41.104771Z","shell.execute_reply.started":"2024-10-15T15:11:39.810635Z","shell.execute_reply":"2024-10-15T15:11:41.103771Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Plot training loss vs steps","metadata":{}},{"cell_type":"code","source":"loss_values = trainer.state.log_history\ntrain_loss = [log['loss'] for log in loss_values if 'loss' in log]\n\nplt.plot(train_loss, label='Training Loss')\nplt.title('Training Loss Over Time')\nplt.xlabel('Training Step')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-15T15:11:41.10638Z","iopub.execute_input":"2024-10-15T15:11:41.106774Z","iopub.status.idle":"2024-10-15T15:11:41.386799Z","shell.execute_reply.started":"2024-10-15T15:11:41.106728Z","shell.execute_reply":"2024-10-15T15:11:41.385826Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This code loads a **fine-tuned language model** with **LoRA adapters**, moves it to the appropriate device (GPU or CPU), and tokenizes a **Tamil prompt**. It then generates a story continuation using the model, applying specific parameters like beam search and sampling to improve the diversity and creativity of the generated text. Finally, the output is decoded back into readable text and printed.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel, PeftConfig\n\n# Check for CUDA availability\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n# Base Model: The code loads the base model \"google/gemma-2b\" (or any other model name if specified). This model is pre-trained (not yet fine-tuned with your custom data). It loads it with bfloat16 precision, which is more efficient on certain hardware, especially for large models.\nbase_model_name = \"google/gemma-2-2b\"  \ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\nbase_model = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.bfloat16)\n\n# Load the LoRA weights\npeft_model_path = \"/kaggle/working/gemma-2b-oscar-tamil-finetuned\"  # Replace with the actual path to your fine-tuned model\nmodel = PeftModel.from_pretrained(base_model, peft_model_path)\n\n# Move the model to the device and set to evaluation mode\nmodel = model.to(device).eval()\nmodel.half()\n# Adjusted prompt for story continuation\nprompt = '''\nஒரு காலத்தில், சின்ன கிராமத்தில் வசித்த ராமு பெரும் கனவுகள் கொண்ட சிறுவன். ஒரு நாள் அவன் ஒரு பெரிய பயணத்தை தொடங்கினான். காடு கடந்து ஒரு புதுமையான நகரத்தை கண்டான். அங்கே அவர் எதிர்கொண்டது...\n\nஇப்போது கதை தொடர்க.\n'''\n\n# Tokenize inputs and move to device\ninputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True)\niinputs = {k: v.half() for k, v in inputs.items()}\n\n# Generate the story with adjusted parameters\ntry:\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_length=512,\n            num_beams=3,\n            do_sample=True,\n            temperature=0.6,\n            top_k=30,\n            top_p=0.95,\n            no_repeat_ngram_size=3,\n            early_stopping=True\n        )\n\n    # Decode and print the generated story\n    generated_story = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(\"Generated Story:\")\n    print(generated_story)\n\nexcept RuntimeError as e:\n    print(f\"An error occurred during generation: {e}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-15T15:43:39.939945Z","iopub.execute_input":"2024-10-15T15:43:39.940835Z","iopub.status.idle":"2024-10-15T15:44:50.110736Z","shell.execute_reply.started":"2024-10-15T15:43:39.940792Z","shell.execute_reply":"2024-10-15T15:44:50.109757Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}